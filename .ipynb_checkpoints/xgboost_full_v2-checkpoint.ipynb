{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d4281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_log = np.log1p(df_train['sale_price'])\n",
    "# Add target to df_train for feature engineering, then drop sale_price\n",
    "df_train['sale_price_log'] = y_log\n",
    "df_train.drop('sale_price', axis=1, inplace=True)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c914a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2 (V3): Even More Aggressive Feature Engineering ---\n",
      "Creating peer-comparison and target-encoded features...\n",
      "Creating deeper interaction and advanced ratio features...\n",
      "Finalizing dataset...\n",
      "Hyper-aggressive feature engineering (V3) complete.\n",
      "Final training features shape: (200000, 98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2 (V3): EVEN MORE AGGRESSIVE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2 (V3): Even More Aggressive Feature Engineering ---\")\n",
    "\n",
    "def create_even_more_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Takes raw train and test dataframes and returns a hyper-rich\n",
    "    set of features ready for modeling.\n",
    "    \"\"\"\n",
    "    # 1. Combine for consistent processing\n",
    "    train_df['is_train'] = 1\n",
    "    test_df['is_train'] = 0\n",
    "    all_data = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "    # 2. Foundational Feature Creation\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "    all_data['total_bathrooms'] = all_data['bath_full'] + 0.5 * all_data['bath_half'] + 0.75 * all_data['bath_3qtr']\n",
    "    all_data['total_sqft'] = all_data['sqft'] + all_data['sqft_fbsmt']\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['time_since_reno'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], all_data['age_at_sale'])\n",
    "\n",
    "    # 3. ADVANCED FEATURE CREATION\n",
    "\n",
    "    # --- A) Location Clusters ---\n",
    "    kmeans = KMeans(n_clusters=30, random_state=RANDOM_STATE, n_init='auto')\n",
    "    all_data['location_cluster'] = kmeans.fit_predict(all_data[['latitude', 'longitude']])\n",
    "    \n",
    "    # --- B) Peer-Comparison & Target-Encoded Features ---\n",
    "    print(\"Creating peer-comparison and target-encoded features...\")\n",
    "    train_copy_for_aggs = all_data[all_data['is_train'] == 1].copy()\n",
    "    group_cols_to_agg = ['location_cluster', 'city', 'submarket']\n",
    "    if 'zipcode' in all_data.columns: group_cols_to_agg.append('zipcode')\n",
    "\n",
    "    for group_col in group_cols_to_agg:\n",
    "        aggs = {\n",
    "            'grade': ['mean', 'std'], 'age_at_sale': ['mean', 'std'],\n",
    "            'total_sqft': ['mean', 'std'], 'sale_price_log': ['mean']\n",
    "        }\n",
    "        group_aggs = train_copy_for_aggs.groupby(group_col).agg(aggs)\n",
    "        group_aggs.columns = [f'{c[0]}_agg_{c[1]}_by_{group_col}' for c in group_aggs.columns]\n",
    "        all_data = all_data.merge(group_aggs, on=group_col, how='left')\n",
    "        all_data[f'grade_vs_mean_{group_col}'] = all_data['grade'] - all_data[f'grade_agg_mean_by_{group_col}']\n",
    "        all_data[f'sqft_vs_mean_{group_col}'] = all_data['total_sqft'] - all_data[f'total_sqft_agg_mean_by_{group_col}']\n",
    "        all_data[f'age_zscore_{group_col}'] = (all_data['age_at_sale'] - all_data[f'age_at_sale_agg_mean_by_{group_col}']) / (all_data[f'age_at_sale_agg_std_by_{group_col}'] + 1e-6)\n",
    "\n",
    "    # --- C) Time-based Trend Feature ---\n",
    "    # Use the mean log price by year as a market trend indicator\n",
    "    all_data['market_trend'] = all_data.groupby('sale_year')['sale_price_log'].transform('mean')\n",
    "    \n",
    "    # --- D) 'sale_warning' Handling ---\n",
    "    sale_warnings_dummies = all_data['sale_warning'].fillna('').str.get_dummies(sep=' ')\n",
    "    top_warnings = sale_warnings_dummies.sum().sort_values(ascending=False).head(15).index\n",
    "    sale_warnings_dummies = sale_warnings_dummies[top_warnings]\n",
    "    sale_warnings_dummies.columns = [f'warning_{col}' for col in top_warnings]\n",
    "    all_data = all_data.join(sale_warnings_dummies)\n",
    "    \n",
    "    # --- E) NEW: Deeper Interaction & Advanced Ratio Features ---\n",
    "    print(\"Creating deeper interaction and advanced ratio features...\")\n",
    "    # Ratios of core values\n",
    "    all_data['imp_val_to_total_val'] = all_data['imp_val'] / (all_data['land_val'] + all_data['imp_val'] + 1e-6)\n",
    "    all_data['land_val_per_sqft_lot'] = all_data['land_val'] / (all_data['sqft_lot'] + 1e-6)\n",
    "    \n",
    "    # Interactions between key features and location/grade context\n",
    "    all_data['grade_x_sqft_vs_mean_cluster'] = all_data['grade'] * all_data['sqft_vs_mean_location_cluster']\n",
    "    all_data['age_x_grade'] = all_data['age_at_sale'] * all_data['grade']\n",
    "    \n",
    "    # Polynomial features on key engineered variables\n",
    "    all_data['total_sqft_sq'] = all_data['total_sqft']**2\n",
    "    all_data['age_at_sale_sq'] = all_data['age_at_sale']**2\n",
    "    \n",
    "    # Cyclical time features\n",
    "    all_data['sin_sale_month'] = np.sin(2 * np.pi * all_data['sale_month']/12)\n",
    "    all_data['cos_sale_month'] = np.cos(2 * np.pi * all_data['sale_month']/12)\n",
    "\n",
    "    # 4. Final Cleanup and Encoding\n",
    "    print(\"Finalizing dataset...\")\n",
    "    for col in all_data.select_dtypes(include='object').columns:\n",
    "        all_data[col] = pd.Categorical(all_data[col]).codes\n",
    "        \n",
    "    cols_to_drop = [\n",
    "        'sale_date', 'year_built', 'year_reno', 'bath_full', 'bath_half',\n",
    "        'bath_3qtr', 'sqft', 'sqft_fbsmt', 'latitude', 'longitude', 'sale_price_log'\n",
    "    ]\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # 5. Separate back into train and test\n",
    "    train_processed = all_data[all_data['is_train'] == 1].drop(columns=['is_train'])\n",
    "    test_processed = all_data[all_data['is_train'] == 0].drop(columns=['is_train'])\n",
    "    \n",
    "    train_cols = train_processed.columns\n",
    "    test_processed = test_processed[train_cols]\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "# Run the new feature engineering pipeline\n",
    "X, X_test = create_even_more_features(df_train, df_test)\n",
    "\n",
    "print(\"Hyper-aggressive feature engineering (V3) complete.\")\n",
    "print(f\"Final training features shape: {X.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b403889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 3: Smart Feature Selection ---\n",
      "Training a temporary LightGBM model to find feature importances...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7244\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score 13.078327\n",
      "\n",
      "Found 4 features with zero importance.\n",
      "Useless features to be dropped: ['view_rainier', 'warning_4', 'warning_44', 'total_sqft_sq']\n",
      "\n",
      "Feature selection complete.\n",
      "Original number of features: 98\n",
      "Number of features after selection: 94\n",
      "\n",
      "Top 30 most important features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sale_year</td>\n",
       "      <td>1566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>land_val</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>imp_val</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sqft_lot</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join_status</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zoning</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sqft_1</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>total_sqft</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>land_val_per_sqft_lot</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>time_since_reno</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>age_x_grade</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>imp_val_to_total_val</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>market_trend</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>grade_vs_mean_location_cluster</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>join_year</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>grade_vs_mean_city</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>age_at_sale</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>grade_vs_mean_submarket</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subdivision</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sale_price_log_agg_mean_by_location_cluster</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>condition</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sale_warning</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>grade</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>sale_price_log_agg_mean_by_submarket</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sale_month</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>age_zscore_city</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>age_zscore_submarket</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>total_bathrooms</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gara_sqft</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature  importance\n",
       "35                                    sale_year        1566\n",
       "9                                      land_val        1111\n",
       "10                                      imp_val         898\n",
       "4                                          area         560\n",
       "11                                     sqft_lot         553\n",
       "2                                   join_status         493\n",
       "6                                        zoning         378\n",
       "12                                       sqft_1         352\n",
       "40                                   total_sqft         342\n",
       "91                        land_val_per_sqft_lot         332\n",
       "42                              time_since_reno         303\n",
       "93                                  age_x_grade         302\n",
       "90                         imp_val_to_total_val         283\n",
       "74                                 market_trend         281\n",
       "51               grade_vs_mean_location_cluster         278\n",
       "3                                     join_year         266\n",
       "61                           grade_vs_mean_city         258\n",
       "38                                  age_at_sale         256\n",
       "71                      grade_vs_mean_submarket         235\n",
       "7                                   subdivision         225\n",
       "50  sale_price_log_agg_mean_by_location_cluster         222\n",
       "15                                    condition         213\n",
       "1                                  sale_warning         211\n",
       "13                                        grade         198\n",
       "70         sale_price_log_agg_mean_by_submarket         173\n",
       "36                                   sale_month         170\n",
       "63                              age_zscore_city         164\n",
       "73                         age_zscore_submarket         153\n",
       "39                              total_bathrooms         151\n",
       "19                                    gara_sqft         150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: SMART FEATURE SELECTION\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 3: Smart Feature Selection ---\")\n",
    "\n",
    "# We will train a single, fast LightGBM model on the full training data\n",
    "# to get a ranked list of feature importances.\n",
    "\n",
    "# Define the model. We use simple parameters as we only care about feature ranking.\n",
    "fs_model = lgb.LGBMRegressor(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Training a temporary LightGBM model to find feature importances...\")\n",
    "# We use 'y_log' which was defined in the first cell\n",
    "fs_model.fit(X, y_log)\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': fs_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# --- Define a threshold for dropping features ---\n",
    "# We will drop features that have zero importance. This is a safe and effective first step.\n",
    "ZERO_IMPORTANCE_THRESHOLD = 0\n",
    "useless_features = importances[importances['importance'] <= ZERO_IMPORTANCE_THRESHOLD]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nFound {len(useless_features)} features with zero importance.\")\n",
    "if len(useless_features) > 0:\n",
    "    print(\"Useless features to be dropped:\", useless_features)\n",
    "\n",
    "# --- Drop the useless features from our datasets ---\n",
    "X_selected = X.drop(columns=useless_features)\n",
    "X_test_selected = X_test.drop(columns=useless_features)\n",
    "\n",
    "# It's also good practice to align columns after dropping\n",
    "X_test_selected = X_test_selected[X_selected.columns]\n",
    "\n",
    "print(f\"\\nFeature selection complete.\")\n",
    "print(f\"Original number of features: {X.shape[1]}\")\n",
    "print(f\"Number of features after selection: {X_selected.shape[1]}\")\n",
    "\n",
    "# --- Clean up memory ---\n",
    "del X, X_test, fs_model\n",
    "gc.collect()\n",
    "\n",
    "# Display the top 30 most important features for our review\n",
    "print(\"\\nTop 30 most important features:\")\n",
    "display(importances.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd475a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: HYPERPARAMETER TUNING & FINAL K-FOLD TRAINING\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 4: Optuna Tuning and Final XGBoost Training ---\")\n",
    "\n",
    "# We need the y_log variable again, which was created in Block 1\n",
    "y_log_for_training = np.log1p(pd.read_csv(DATA_PATH + 'dataset.csv')['sale_price'])\n",
    "\n",
    "# Define the Winkler Score function for our objective\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    width = upper - lower\n",
    "    penalty_lower = (2 / alpha) * (lower - y_true) * (y_true < lower)\n",
    "    penalty_upper = (2 / alpha) * (y_true - upper) * (y_true > upper)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- 1. Optuna Objective Function ---\n",
    "# This function trains a model with a given set of hyperparameters and returns its score\n",
    "def objective(trial):\n",
    "    # We will use a single train/validation split for speed during the search\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        X_selected, y_log_for_training, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    # Train lower and upper models\n",
    "    preds_lower, preds_upper = np.zeros(len(val_y)), np.zeros(len(val_y))\n",
    "    for i, alpha in enumerate([0.05, 0.95]):\n",
    "        model = xgb.XGBRegressor(\n",
    "            **params, n_estimators=1000, quantile_alpha=alpha, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        )\n",
    "        model.fit(train_x, train_y, eval_set=[(val_x, val_y)], early_stopping_rounds=50, verbose=False)\n",
    "        if i == 0:\n",
    "            preds_lower = model.predict(val_x)\n",
    "        else:\n",
    "            preds_upper = model.predict(val_x)\n",
    "    \n",
    "    # Calculate Winkler score\n",
    "    true_vals = np.expm1(val_y)\n",
    "    lower_vals = np.expm1(preds_lower)\n",
    "    upper_vals = np.expm1(preds_upper)\n",
    "    upper_vals = np.maximum(lower_vals, upper_vals)\n",
    "    \n",
    "    return winkler_score(true_vals, lower_vals, upper_vals)\n",
    "\n",
    "# --- 2. Run the Optuna Study ---\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# Run for 30 trials. Increase for more exhaustive search.\n",
    "study.optimize(objective, n_trials=30, n_jobs=1) # n_jobs=1 is safer for nested parallelization\n",
    "best_params = study.best_params\n",
    "print(\"Optuna search complete. Best parameters found.\")\n",
    "print(best_params)\n",
    "\n",
    "# --- 3. Final K-Fold Training with Best Parameters ---\n",
    "print(\"\\nStarting final K-Fold training with optimized parameters...\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_preds = np.zeros((len(X_selected), 2))\n",
    "test_preds = np.zeros((len(X_test_selected), 2))\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "dtest = xgb.DMatrix(X_test_selected)\n",
    "\n",
    "final_xgb_params = {\n",
    "    'objective': 'reg:quantileerror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "    'random_state': RANDOM_STATE, 'n_jobs': -1, **best_params\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, grade_for_stratify)):\n",
    "    print(f\"===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "    X_train_fold, y_train_fold = X_selected.iloc[train_idx], y_log_for_training.iloc[train_idx]\n",
    "    X_val_fold, y_val_fold = X_selected.iloc[val_idx], y_log_for_training.iloc[val_idx]\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "    \n",
    "    for i, alpha in enumerate([0.05, 0.95]):\n",
    "        print(f\"Training alpha={alpha}...\")\n",
    "        current_params = final_xgb_params.copy()\n",
    "        current_params['quantile_alpha'] = alpha\n",
    "        model = xgb.train(\n",
    "            params=current_params, dtrain=dtrain, num_boost_round=2000,\n",
    "            evals=[(dval, 'validation')], early_stopping_rounds=100, verbose_eval=False\n",
    "        )\n",
    "        oof_preds[val_idx, i] = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "        test_preds[:, i] += model.predict(dtest, iteration_range=(0, model.best_iteration)) / N_SPLITS\n",
    "\n",
    "print(\"\\nFinal K-Fold training complete.\")\n",
    "\n",
    "# --- 4. Final Calibration and Submission ---\n",
    "print(\"\\nCalibrating and creating final submission file...\")\n",
    "y_true_final = np.expm1(y_log_for_training)\n",
    "oof_lower = np.expm1(oof_preds[:, 0])\n",
    "oof_upper = np.expm1(oof_preds[:, 1])\n",
    "oof_upper = np.maximum(oof_lower, oof_upper)\n",
    "score, coverage = winkler_score(y_true_final, oof_lower, oof_upper, return_coverage=True)\n",
    "print(f\"Final OOF Winkler Score (before calib): {score:,.2f} | Coverage: {coverage:.2%}\")\n",
    "\n",
    "best_factor = 1.0\n",
    "best_coverage_diff = abs(coverage - 0.90)\n",
    "for factor in np.arange(0.9, 1.2, 0.001):\n",
    "    center = (oof_lower + oof_upper) / 2\n",
    "    width = oof_upper - oof_lower\n",
    "    new_lower = center - (width / 2) * factor\n",
    "    new_upper = center + (width / 2) * factor\n",
    "    _, current_coverage = winkler_score(y_true_final, new_lower, new_upper, return_coverage=True)\n",
    "    if abs(current_coverage - 0.90) < best_coverage_diff:\n",
    "        best_coverage_diff = abs(current_coverage - 0.90)\n",
    "        best_factor = factor\n",
    "print(f\"Best calibration factor found: {best_factor:.3f}\")\n",
    "\n",
    "test_lower = np.expm1(test_preds[:, 0])\n",
    "test_upper = np.expm1(test_preds[:, 1])\n",
    "test_center = (test_lower + test_upper) / 2\n",
    "test_width = test_upper - test_lower\n",
    "calibrated_lower = test_center - (test_width / 2) * best_factor\n",
    "calibrated_upper = test_center + (test_width / 2) * best_factor\n",
    "calibrated_upper = np.maximum(calibrated_lower, calibrated_upper)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': X_test_selected.index,\n",
    "    'pi_lower': calibrated_lower,\n",
    "    'pi_upper': calibrated_upper\n",
    "})\n",
    "submission_df.to_csv('submission_tuned_aggressive.csv', index=False)\n",
    "print(\"\\n'submission_tuned_aggressive.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-houseprice",
   "language": "python",
   "name": "kaggle-houseprice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
