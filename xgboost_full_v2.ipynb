{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2edb0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_log = np.log1p(df_train['sale_price'])\n",
    "# Add target to df_train for feature engineering, then drop sale_price\n",
    "df_train['sale_price_log'] = y_log\n",
    "df_train.drop('sale_price', axis=1, inplace=True)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d501b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2 (V3): Even More Aggressive Feature Engineering ---\n",
      "Creating peer-comparison and target-encoded features...\n",
      "Creating deeper interaction and advanced ratio features...\n",
      "Finalizing dataset...\n",
      "Hyper-aggressive feature engineering (V3) complete.\n",
      "Final training features shape: (200000, 98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2 (V3): EVEN MORE AGGRESSIVE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2 (V3): Even More Aggressive Feature Engineering ---\")\n",
    "\n",
    "def create_even_more_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Takes raw train and test dataframes and returns a hyper-rich\n",
    "    set of features ready for modeling.\n",
    "    \"\"\"\n",
    "    # 1. Combine for consistent processing\n",
    "    train_df['is_train'] = 1\n",
    "    test_df['is_train'] = 0\n",
    "    all_data = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "    # 2. Foundational Feature Creation\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "    all_data['total_bathrooms'] = all_data['bath_full'] + 0.5 * all_data['bath_half'] + 0.75 * all_data['bath_3qtr']\n",
    "    all_data['total_sqft'] = all_data['sqft'] + all_data['sqft_fbsmt']\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['time_since_reno'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], all_data['age_at_sale'])\n",
    "\n",
    "    # 3. ADVANCED FEATURE CREATION\n",
    "\n",
    "    # --- A) Location Clusters ---\n",
    "    kmeans = KMeans(n_clusters=30, random_state=RANDOM_STATE, n_init='auto')\n",
    "    all_data['location_cluster'] = kmeans.fit_predict(all_data[['latitude', 'longitude']])\n",
    "    \n",
    "    # --- B) Peer-Comparison & Target-Encoded Features ---\n",
    "    print(\"Creating peer-comparison and target-encoded features...\")\n",
    "    train_copy_for_aggs = all_data[all_data['is_train'] == 1].copy()\n",
    "    group_cols_to_agg = ['location_cluster', 'city', 'submarket']\n",
    "    if 'zipcode' in all_data.columns: group_cols_to_agg.append('zipcode')\n",
    "\n",
    "    for group_col in group_cols_to_agg:\n",
    "        aggs = {\n",
    "            'grade': ['mean', 'std'], 'age_at_sale': ['mean', 'std'],\n",
    "            'total_sqft': ['mean', 'std'], 'sale_price_log': ['mean']\n",
    "        }\n",
    "        group_aggs = train_copy_for_aggs.groupby(group_col).agg(aggs)\n",
    "        group_aggs.columns = [f'{c[0]}_agg_{c[1]}_by_{group_col}' for c in group_aggs.columns]\n",
    "        all_data = all_data.merge(group_aggs, on=group_col, how='left')\n",
    "        all_data[f'grade_vs_mean_{group_col}'] = all_data['grade'] - all_data[f'grade_agg_mean_by_{group_col}']\n",
    "        all_data[f'sqft_vs_mean_{group_col}'] = all_data['total_sqft'] - all_data[f'total_sqft_agg_mean_by_{group_col}']\n",
    "        all_data[f'age_zscore_{group_col}'] = (all_data['age_at_sale'] - all_data[f'age_at_sale_agg_mean_by_{group_col}']) / (all_data[f'age_at_sale_agg_std_by_{group_col}'] + 1e-6)\n",
    "\n",
    "    # --- C) Time-based Trend Feature ---\n",
    "    # Use the mean log price by year as a market trend indicator\n",
    "    all_data['market_trend'] = all_data.groupby('sale_year')['sale_price_log'].transform('mean')\n",
    "    \n",
    "    # --- D) 'sale_warning' Handling ---\n",
    "    sale_warnings_dummies = all_data['sale_warning'].fillna('').str.get_dummies(sep=' ')\n",
    "    top_warnings = sale_warnings_dummies.sum().sort_values(ascending=False).head(15).index\n",
    "    sale_warnings_dummies = sale_warnings_dummies[top_warnings]\n",
    "    sale_warnings_dummies.columns = [f'warning_{col}' for col in top_warnings]\n",
    "    all_data = all_data.join(sale_warnings_dummies)\n",
    "    \n",
    "    # --- E) NEW: Deeper Interaction & Advanced Ratio Features ---\n",
    "    print(\"Creating deeper interaction and advanced ratio features...\")\n",
    "    # Ratios of core values\n",
    "    all_data['imp_val_to_total_val'] = all_data['imp_val'] / (all_data['land_val'] + all_data['imp_val'] + 1e-6)\n",
    "    all_data['land_val_per_sqft_lot'] = all_data['land_val'] / (all_data['sqft_lot'] + 1e-6)\n",
    "    \n",
    "    # Interactions between key features and location/grade context\n",
    "    all_data['grade_x_sqft_vs_mean_cluster'] = all_data['grade'] * all_data['sqft_vs_mean_location_cluster']\n",
    "    all_data['age_x_grade'] = all_data['age_at_sale'] * all_data['grade']\n",
    "    \n",
    "    # Polynomial features on key engineered variables\n",
    "    all_data['total_sqft_sq'] = all_data['total_sqft']**2\n",
    "    all_data['age_at_sale_sq'] = all_data['age_at_sale']**2\n",
    "    \n",
    "    # Cyclical time features\n",
    "    all_data['sin_sale_month'] = np.sin(2 * np.pi * all_data['sale_month']/12)\n",
    "    all_data['cos_sale_month'] = np.cos(2 * np.pi * all_data['sale_month']/12)\n",
    "\n",
    "    # 4. Final Cleanup and Encoding\n",
    "    print(\"Finalizing dataset...\")\n",
    "    for col in all_data.select_dtypes(include='object').columns:\n",
    "        all_data[col] = pd.Categorical(all_data[col]).codes\n",
    "        \n",
    "    cols_to_drop = [\n",
    "        'sale_date', 'year_built', 'year_reno', 'bath_full', 'bath_half',\n",
    "        'bath_3qtr', 'sqft', 'sqft_fbsmt', 'latitude', 'longitude', 'sale_price_log'\n",
    "    ]\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # 5. Separate back into train and test\n",
    "    train_processed = all_data[all_data['is_train'] == 1].drop(columns=['is_train'])\n",
    "    test_processed = all_data[all_data['is_train'] == 0].drop(columns=['is_train'])\n",
    "    \n",
    "    train_cols = train_processed.columns\n",
    "    test_processed = test_processed[train_cols]\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "# Run the new feature engineering pipeline\n",
    "X, X_test = create_even_more_features(df_train, df_test)\n",
    "\n",
    "print(\"Hyper-aggressive feature engineering (V3) complete.\")\n",
    "print(f\"Final training features shape: {X.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4772d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 3: Smart Feature Selection ---\n",
      "Training a temporary LightGBM model to find feature importances...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7244\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score 13.078327\n",
      "\n",
      "Found 4 features with zero importance.\n",
      "Useless features to be dropped: ['view_rainier', 'warning_4', 'warning_44', 'total_sqft_sq']\n",
      "\n",
      "Feature selection complete.\n",
      "Original number of features: 98\n",
      "Number of features after selection: 94\n",
      "\n",
      "Top 30 most important features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sale_year</td>\n",
       "      <td>1566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>land_val</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>imp_val</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sqft_lot</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join_status</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zoning</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sqft_1</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>total_sqft</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>land_val_per_sqft_lot</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>time_since_reno</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>age_x_grade</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>imp_val_to_total_val</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>market_trend</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>grade_vs_mean_location_cluster</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>join_year</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>grade_vs_mean_city</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>age_at_sale</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>grade_vs_mean_submarket</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subdivision</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sale_price_log_agg_mean_by_location_cluster</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>condition</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sale_warning</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>grade</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>sale_price_log_agg_mean_by_submarket</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sale_month</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>age_zscore_city</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>age_zscore_submarket</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>total_bathrooms</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gara_sqft</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature  importance\n",
       "35                                    sale_year        1566\n",
       "9                                      land_val        1111\n",
       "10                                      imp_val         898\n",
       "4                                          area         560\n",
       "11                                     sqft_lot         553\n",
       "2                                   join_status         493\n",
       "6                                        zoning         378\n",
       "12                                       sqft_1         352\n",
       "40                                   total_sqft         342\n",
       "91                        land_val_per_sqft_lot         332\n",
       "42                              time_since_reno         303\n",
       "93                                  age_x_grade         302\n",
       "90                         imp_val_to_total_val         283\n",
       "74                                 market_trend         281\n",
       "51               grade_vs_mean_location_cluster         278\n",
       "3                                     join_year         266\n",
       "61                           grade_vs_mean_city         258\n",
       "38                                  age_at_sale         256\n",
       "71                      grade_vs_mean_submarket         235\n",
       "7                                   subdivision         225\n",
       "50  sale_price_log_agg_mean_by_location_cluster         222\n",
       "15                                    condition         213\n",
       "1                                  sale_warning         211\n",
       "13                                        grade         198\n",
       "70         sale_price_log_agg_mean_by_submarket         173\n",
       "36                                   sale_month         170\n",
       "63                              age_zscore_city         164\n",
       "73                         age_zscore_submarket         153\n",
       "39                              total_bathrooms         151\n",
       "19                                    gara_sqft         150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: SMART FEATURE SELECTION\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 3: Smart Feature Selection ---\")\n",
    "\n",
    "# We will train a single, fast LightGBM model on the full training data\n",
    "# to get a ranked list of feature importances.\n",
    "\n",
    "# Define the model. We use simple parameters as we only care about feature ranking.\n",
    "fs_model = lgb.LGBMRegressor(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Training a temporary LightGBM model to find feature importances...\")\n",
    "# We use 'y_log' which was defined in the first cell\n",
    "fs_model.fit(X, y_log)\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': fs_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# --- Define a threshold for dropping features ---\n",
    "# We will drop features that have zero importance. This is a safe and effective first step.\n",
    "ZERO_IMPORTANCE_THRESHOLD = 0\n",
    "useless_features = importances[importances['importance'] <= ZERO_IMPORTANCE_THRESHOLD]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nFound {len(useless_features)} features with zero importance.\")\n",
    "if len(useless_features) > 0:\n",
    "    print(\"Useless features to be dropped:\", useless_features)\n",
    "\n",
    "# --- Drop the useless features from our datasets ---\n",
    "X_selected = X.drop(columns=useless_features)\n",
    "X_test_selected = X_test.drop(columns=useless_features)\n",
    "\n",
    "# It's also good practice to align columns after dropping\n",
    "X_test_selected = X_test_selected[X_selected.columns]\n",
    "\n",
    "print(f\"\\nFeature selection complete.\")\n",
    "print(f\"Original number of features: {X.shape[1]}\")\n",
    "print(f\"Number of features after selection: {X_selected.shape[1]}\")\n",
    "\n",
    "# --- Clean up memory ---\n",
    "del X, X_test, fs_model\n",
    "gc.collect()\n",
    "\n",
    "# Display the top 30 most important features for our review\n",
    "print(\"\\nTop 30 most important features:\")\n",
    "display(importances.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7127b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-03 18:26:40,335] A new study created in memory with name: no-name-cd180c40-655c-4aed-8055-a5b239c39507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 4: Optuna Tuning and Final XGBoost Training ---\n",
      "Starting Optuna hyperparameter search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-03 18:29:17,562] Trial 0 finished with value: 350090.3434472656 and parameters: {'eta': 0.021258974919153073, 'max_depth': 5, 'subsample': 0.6870491256717616, 'colsample_bytree': 0.6663144841103836, 'min_child_weight': 3, 'gamma': 0.21561539357653073}. Best is trial 0 with value: 350090.3434472656.\n",
      "[I 2025-07-03 18:30:57,451] Trial 1 finished with value: 354381.2310390625 and parameters: {'eta': 0.013960201471766374, 'max_depth': 8, 'subsample': 0.7969621608870954, 'colsample_bytree': 0.6790056528639268, 'min_child_weight': 9, 'gamma': 0.028145789513710735}. Best is trial 0 with value: 350090.3434472656.\n",
      "[I 2025-07-03 18:32:20,986] Trial 2 finished with value: 344766.2009482422 and parameters: {'eta': 0.021034732901866842, 'max_depth': 6, 'subsample': 0.8821774024308269, 'colsample_bytree': 0.6385971010352484, 'min_child_weight': 6, 'gamma': 1.8042355552698947e-06}. Best is trial 2 with value: 344766.2009482422.\n",
      "[I 2025-07-03 18:33:52,191] Trial 3 finished with value: 365086.3145574219 and parameters: {'eta': 0.055582422957658875, 'max_depth': 9, 'subsample': 0.7159891046825237, 'colsample_bytree': 0.7790482223536448, 'min_child_weight': 5, 'gamma': 3.558024889218012e-08}. Best is trial 2 with value: 344766.2009482422.\n",
      "[I 2025-07-03 18:35:09,473] Trial 4 finished with value: 344736.36276660155 and parameters: {'eta': 0.03883987446760088, 'max_depth': 7, 'subsample': 0.7703416504961098, 'colsample_bytree': 0.6995061925019819, 'min_child_weight': 1, 'gamma': 0.00019733991144413977}. Best is trial 4 with value: 344736.36276660155.\n",
      "[I 2025-07-03 18:36:20,722] Trial 5 finished with value: 340831.0240918945 and parameters: {'eta': 0.07241666867489689, 'max_depth': 6, 'subsample': 0.6912837163279031, 'colsample_bytree': 0.737932638363876, 'min_child_weight': 9, 'gamma': 8.552438438201446e-07}. Best is trial 5 with value: 340831.0240918945.\n",
      "[I 2025-07-03 18:37:28,924] Trial 6 finished with value: 349440.81980722654 and parameters: {'eta': 0.03754069838594373, 'max_depth': 6, 'subsample': 0.7346288929468239, 'colsample_bytree': 0.8618502484062939, 'min_child_weight': 2, 'gamma': 0.5962856715773425}. Best is trial 5 with value: 340831.0240918945.\n",
      "[I 2025-07-03 18:39:00,522] Trial 7 finished with value: 338359.38111015625 and parameters: {'eta': 0.037995733175453364, 'max_depth': 6, 'subsample': 0.7363277435747415, 'colsample_bytree': 0.6029498160531425, 'min_child_weight': 6, 'gamma': 4.774795869345142e-07}. Best is trial 7 with value: 338359.38111015625.\n",
      "[I 2025-07-03 18:40:01,793] Trial 8 finished with value: 338047.4597049805 and parameters: {'eta': 0.09713748282518722, 'max_depth': 4, 'subsample': 0.6029740881918924, 'colsample_bytree': 0.706493823925173, 'min_child_weight': 6, 'gamma': 0.00010938114609888715}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:40:41,724] Trial 9 finished with value: 342636.7901083984 and parameters: {'eta': 0.07813397096300777, 'max_depth': 6, 'subsample': 0.7532840941767044, 'colsample_bytree': 0.7618780877294637, 'min_child_weight': 9, 'gamma': 9.580347855102773e-07}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:41:13,974] Trial 10 finished with value: 338658.2084459961 and parameters: {'eta': 0.09557879637246497, 'max_depth': 4, 'subsample': 0.6003479162117399, 'colsample_bytree': 0.8376207646046546, 'min_child_weight': 4, 'gamma': 0.0007424420347551653}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:41:44,311] Trial 11 finished with value: 339900.31921171874 and parameters: {'eta': 0.05350387492230811, 'max_depth': 4, 'subsample': 0.6050768699658002, 'colsample_bytree': 0.6041459732935748, 'min_child_weight': 7, 'gamma': 3.815274553341903e-05}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:42:27,086] Trial 12 finished with value: 369099.28190644534 and parameters: {'eta': 0.02557708386527625, 'max_depth': 10, 'subsample': 0.8349244133826677, 'colsample_bytree': 0.6192757680914996, 'min_child_weight': 7, 'gamma': 0.005299197935727858}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:42:55,438] Trial 13 finished with value: 407809.93943320314 and parameters: {'eta': 0.010340049542113079, 'max_depth': 4, 'subsample': 0.6467874335012046, 'colsample_bytree': 0.7098504572271123, 'min_child_weight': 7, 'gamma': 2.795964397011818e-08}. Best is trial 8 with value: 338047.4597049805.\n",
      "[I 2025-07-03 18:43:33,829] Trial 14 finished with value: 335777.47588320315 and parameters: {'eta': 0.051042540748323394, 'max_depth': 5, 'subsample': 0.6547329533842997, 'colsample_bytree': 0.8229177174462796, 'min_child_weight': 5, 'gamma': 1.9338620790157364e-05}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:44:04,961] Trial 15 finished with value: 336525.34582871094 and parameters: {'eta': 0.05559721945251021, 'max_depth': 5, 'subsample': 0.6433838504922957, 'colsample_bytree': 0.8124440567457742, 'min_child_weight': 4, 'gamma': 1.9729443301945903e-05}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:44:37,615] Trial 16 finished with value: 337103.6785871094 and parameters: {'eta': 0.05451215396909399, 'max_depth': 5, 'subsample': 0.6522445350789727, 'colsample_bytree': 0.8096344017951734, 'min_child_weight': 4, 'gamma': 1.5489542481481648e-05}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:45:09,034] Trial 17 finished with value: 336973.2959673828 and parameters: {'eta': 0.0478308834994554, 'max_depth': 5, 'subsample': 0.6526013054667729, 'colsample_bytree': 0.874817472226638, 'min_child_weight': 4, 'gamma': 8.664364865487648e-06}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:45:42,698] Trial 18 finished with value: 341518.9274011719 and parameters: {'eta': 0.028156328736412516, 'max_depth': 7, 'subsample': 0.6928818513344216, 'colsample_bytree': 0.810318248943638, 'min_child_weight': 2, 'gamma': 0.0022436812941903465}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:46:19,276] Trial 19 finished with value: 354925.1760164063 and parameters: {'eta': 0.06887859513856802, 'max_depth': 8, 'subsample': 0.6511122556643459, 'colsample_bytree': 0.8881602445093517, 'min_child_weight': 5, 'gamma': 7.155971009400196e-06}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:46:52,482] Trial 20 finished with value: 337340.06363691407 and parameters: {'eta': 0.04341731274360946, 'max_depth': 5, 'subsample': 0.6295001946066504, 'colsample_bytree': 0.8002231834926382, 'min_child_weight': 3, 'gamma': 1.4940460100071393e-07}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:47:24,746] Trial 21 finished with value: 337330.45552773436 and parameters: {'eta': 0.04820764005985028, 'max_depth': 5, 'subsample': 0.6586078288846344, 'colsample_bytree': 0.8626100417431498, 'min_child_weight': 4, 'gamma': 6.31885642006459e-06}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:47:57,026] Trial 22 finished with value: 337273.5712085937 and parameters: {'eta': 0.06358928897508402, 'max_depth': 5, 'subsample': 0.6856160405820801, 'colsample_bytree': 0.8916491208640732, 'min_child_weight': 3, 'gamma': 6.450447301642202e-05}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:48:31,623] Trial 23 finished with value: 340283.3337185547 and parameters: {'eta': 0.03186200879284968, 'max_depth': 5, 'subsample': 0.6734943999818201, 'colsample_bytree': 0.8446857421647405, 'min_child_weight': 5, 'gamma': 0.0003388496402700014}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:49:03,122] Trial 24 finished with value: 342753.53102167975 and parameters: {'eta': 0.045820415351936525, 'max_depth': 7, 'subsample': 0.6294569465312092, 'colsample_bytree': 0.8220536181458873, 'min_child_weight': 4, 'gamma': 4.056144151264692e-06}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:49:37,644] Trial 25 finished with value: 337323.17106376955 and parameters: {'eta': 0.08289835338707997, 'max_depth': 4, 'subsample': 0.7151676684671177, 'colsample_bytree': 0.7811062588694542, 'min_child_weight': 2, 'gamma': 2.403119600404462e-05}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:50:08,619] Trial 26 finished with value: 336889.9528044922 and parameters: {'eta': 0.06201329169168845, 'max_depth': 5, 'subsample': 0.6289811969175365, 'colsample_bytree': 0.8587158084040216, 'min_child_weight': 8, 'gamma': 0.0008977458832866617}. Best is trial 14 with value: 335777.47588320315.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-03 18:50:43,077] Trial 27 finished with value: 339879.7123726563 and parameters: {'eta': 0.0630656717010009, 'max_depth': 6, 'subsample': 0.623526097100019, 'colsample_bytree': 0.8377357573955586, 'min_child_weight': 10, 'gamma': 0.012190294594933164}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:51:16,638] Trial 28 finished with value: 343783.80724863283 and parameters: {'eta': 0.05712403187481595, 'max_depth': 7, 'subsample': 0.6346845835272922, 'colsample_bytree': 0.7823895635055007, 'min_child_weight': 8, 'gamma': 0.001136635340957706}. Best is trial 14 with value: 335777.47588320315.\n",
      "[I 2025-07-03 18:51:47,831] Trial 29 finished with value: 361018.3852300781 and parameters: {'eta': 0.022726606528472484, 'max_depth': 4, 'subsample': 0.6741587387027982, 'colsample_bytree': 0.7405435378015338, 'min_child_weight': 8, 'gamma': 0.02430736016569751}. Best is trial 14 with value: 335777.47588320315.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna search complete. Best parameters found.\n",
      "{'eta': 0.051042540748323394, 'max_depth': 5, 'subsample': 0.6547329533842997, 'colsample_bytree': 0.8229177174462796, 'min_child_weight': 5, 'gamma': 1.9338620790157364e-05}\n",
      "\n",
      "Starting final K-Fold training with optimized parameters...\n",
      "===== FOLD 1/5 =====\n",
      "Training alpha=0.05...\n",
      "Training alpha=0.95...\n",
      "===== FOLD 2/5 =====\n",
      "Training alpha=0.05...\n",
      "Training alpha=0.95...\n",
      "===== FOLD 3/5 =====\n",
      "Training alpha=0.05...\n",
      "Training alpha=0.95...\n",
      "===== FOLD 4/5 =====\n",
      "Training alpha=0.05...\n",
      "Training alpha=0.95...\n",
      "===== FOLD 5/5 =====\n",
      "Training alpha=0.05...\n",
      "Training alpha=0.95...\n",
      "\n",
      "Final K-Fold training complete.\n",
      "\n",
      "Calibrating and creating final submission file...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "winkler_score() got an unexpected keyword argument 'return_coverage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m oof_upper = np.expm1(oof_preds[:, \u001b[32m1\u001b[39m])\n\u001b[32m    107\u001b[39m oof_upper = np.maximum(oof_lower, oof_upper)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m score, coverage = \u001b[43mwinkler_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moof_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moof_upper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_coverage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal OOF Winkler Score (before calib): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Coverage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverage\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m best_factor = \u001b[32m1.0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: winkler_score() got an unexpected keyword argument 'return_coverage'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4 (FINAL): OPTUNA TUNING & K-FOLD TRAINING (FUNCTIONAL API)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 4: Optuna Tuning and Final XGBoost Training ---\")\n",
    "\n",
    "# --- Define the Winkler Score function ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    width = upper - lower\n",
    "    penalty_lower = (2 / alpha) * (lower - y_true) * (y_true < lower)\n",
    "    penalty_upper = (2 / alpha) * (y_true - upper) * (y_true > upper)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- 1. Optuna Objective Function (using xgb.train) ---\n",
    "def objective(trial):\n",
    "    # Use a single train/validation split for speed during the search\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        X_selected, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    # Convert to DMatrix for functional API\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dval = xgb.DMatrix(val_x, label=val_y)\n",
    "    \n",
    "    # Define search space\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    # Train lower and upper models\n",
    "    preds_lower = np.zeros(len(val_y))\n",
    "    preds_upper = np.zeros(len(val_y))\n",
    "    \n",
    "    # Lower model\n",
    "    params_lower = params.copy()\n",
    "    params_lower['quantile_alpha'] = 0.05\n",
    "    model_lower = xgb.train(params_lower, dtrain, num_boost_round=1000,\n",
    "                            evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "    preds_lower = model_lower.predict(dval, iteration_range=(0, model_lower.best_iteration))\n",
    "\n",
    "    # Upper model\n",
    "    params_upper = params.copy()\n",
    "    params_upper['quantile_alpha'] = 0.95\n",
    "    model_upper = xgb.train(params_upper, dtrain, num_boost_round=1000,\n",
    "                            evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "    preds_upper = model_upper.predict(dval, iteration_range=(0, model_upper.best_iteration))\n",
    "    \n",
    "    # Calculate Winkler score\n",
    "    true_vals = np.expm1(val_y)\n",
    "    lower_vals = np.expm1(preds_lower)\n",
    "    upper_vals = np.expm1(preds_upper)\n",
    "    upper_vals = np.maximum(lower_vals, upper_vals)\n",
    "    \n",
    "    return winkler_score(true_vals, lower_vals, upper_vals)\n",
    "\n",
    "# --- 2. Run the Optuna Study ---\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30, n_jobs=1) \n",
    "best_params = study.best_params\n",
    "print(\"Optuna search complete. Best parameters found.\")\n",
    "print(best_params)\n",
    "\n",
    "# --- 3. Final K-Fold Training with Best Parameters ---\n",
    "print(\"\\nStarting final K-Fold training with optimized parameters...\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_preds = np.zeros((len(X_selected), 2))\n",
    "test_preds = np.zeros((len(X_test_selected), 2))\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "dtest = xgb.DMatrix(X_test_selected)\n",
    "\n",
    "final_xgb_params = {\n",
    "    'objective': 'reg:quantileerror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "    'random_state': RANDOM_STATE, 'n_jobs': -1, **best_params\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, grade_for_stratify)):\n",
    "    print(f\"===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "    X_train_fold, y_train_fold = X_selected.iloc[train_idx], y_log.iloc[train_idx]\n",
    "    X_val_fold, y_val_fold = X_selected.iloc[val_idx], y_log.iloc[val_idx]\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "    \n",
    "    for i, alpha in enumerate([0.05, 0.95]):\n",
    "        print(f\"Training alpha={alpha}...\")\n",
    "        current_params = final_xgb_params.copy()\n",
    "        current_params['quantile_alpha'] = alpha\n",
    "        model = xgb.train(\n",
    "            params=current_params, dtrain=dtrain, num_boost_round=2000,\n",
    "            evals=[(dval, 'validation')], early_stopping_rounds=100, verbose_eval=False\n",
    "        )\n",
    "        oof_preds[val_idx, i] = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "        test_preds[:, i] += model.predict(dtest, iteration_range=(0, model.best_iteration)) / N_SPLITS\n",
    "\n",
    "print(\"\\nFinal K-Fold training complete.\")\n",
    "\n",
    "# --- 4. Final Calibration and Submission ---\n",
    "print(\"\\nCalibrating and creating final submission file...\")\n",
    "y_true_final = np.expm1(y_log)\n",
    "oof_lower = np.expm1(oof_preds[:, 0])\n",
    "oof_upper = np.expm1(oof_preds[:, 1])\n",
    "oof_upper = np.maximum(oof_lower, oof_upper)\n",
    "score, coverage = winkler_score(y_true_final, oof_lower, oof_upper, return_coverage=True)\n",
    "print(f\"Final OOF Winkler Score (before calib): {score:,.2f} | Coverage: {coverage:.2%}\")\n",
    "\n",
    "best_factor = 1.0\n",
    "best_coverage_diff = abs(coverage - 0.90)\n",
    "for factor in np.arange(0.9, 1.2, 0.001):\n",
    "    center = (oof_lower + oof_upper) / 2\n",
    "    width = oof_upper - oof_lower\n",
    "    new_lower = center - (width / 2) * factor\n",
    "    new_upper = center + (width / 2) * factor\n",
    "    _, current_coverage = winkler_score(y_true_final, new_lower, new_upper, return_coverage=True)\n",
    "    if abs(current_coverage - 0.90) < best_coverage_diff:\n",
    "        best_coverage_diff = abs(current_coverage - 0.90)\n",
    "        best_factor = factor\n",
    "print(f\"Best calibration factor found: {best_factor:.3f}\")\n",
    "\n",
    "test_lower = np.expm1(test_preds[:, 0])\n",
    "test_upper = np.expm1(test_preds[:, 1])\n",
    "test_center = (test_lower + test_upper) / 2\n",
    "test_width = test_upper - test_lower\n",
    "calibrated_lower = test_center - (test_width / 2) * best_factor\n",
    "calibrated_upper = test_center + (test_width / 2) * best_factor\n",
    "calibrated_upper = np.maximum(calibrated_lower, calibrated_upper)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': X_test_selected.index,\n",
    "    'pi_lower': calibrated_lower,\n",
    "    'pi_upper': calibrated_upper\n",
    "})\n",
    "submission_df.to_csv('submission_tuned_aggressive.csv', index=False)\n",
    "print(\"\\n'submission_tuned_aggressive.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dea9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Calibration and Submission ---\n",
      "\n",
      "Final OOF Winkler Score (before calibration): 336,308.55\n",
      "Final OOF Coverage (before calibration):    86.48%\n",
      "\n",
      "Searching for best calibration factor to target 90% coverage...\n",
      "Best calibration factor found: 1.106\n",
      "\n",
      "Creating final submission file...\n",
      "Applying calibration factor (1.106) to test predictions...\n",
      "\n",
      "'submission_tuned_aggressive_final.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>799381.338780</td>\n",
       "      <td>1.156328e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>546260.163649</td>\n",
       "      <td>8.059828e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>455234.882498</td>\n",
       "      <td>7.022682e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>291037.632249</td>\n",
       "      <td>4.547403e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>391268.244785</td>\n",
       "      <td>7.423563e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  799381.338780  1.156328e+06\n",
       "1  200001  546260.163649  8.059828e+05\n",
       "2  200002  455234.882498  7.022682e+05\n",
       "3  200003  291037.632249  4.547403e+05\n",
       "4  200004  391268.244785  7.423563e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL BLOCK: CALIBRATION AND SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"--- Starting Final Calibration and Submission ---\")\n",
    "\n",
    "# --- Define the Correct Winkler Score function ---\n",
    "# This version includes the 'return_coverage' argument.\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    # Use np.where for clarity and safety instead of boolean multiplication\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    \n",
    "    if return_coverage:\n",
    "        inside = (y_true >= lower) & (y_true <= upper)\n",
    "        coverage = np.mean(inside)\n",
    "        return np.mean(score), coverage\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "# --- 1. Evaluate the Out-of-Fold (OOF) Predictions ---\n",
    "# The y_log variable should still be in memory from Block 1.\n",
    "# If not, uncomment the line below.\n",
    "# y_log = np.log1p(pd.read_csv(DATA_PATH + 'dataset.csv')['sale_price'])\n",
    "\n",
    "y_true_final = np.expm1(y_log)\n",
    "oof_lower = np.expm1(oof_preds[:, 0])\n",
    "oof_upper = np.expm1(oof_preds[:, 1])\n",
    "oof_upper = np.maximum(oof_lower, oof_upper)\n",
    "\n",
    "# Calculate the final validation score and coverage\n",
    "final_score, final_coverage = winkler_score(y_true_final, oof_lower, oof_upper, alpha=0.1, return_coverage=True)\n",
    "print(f\"\\nFinal OOF Winkler Score (before calibration): {final_score:,.2f}\")\n",
    "print(f\"Final OOF Coverage (before calibration):    {final_coverage:.2%}\")\n",
    "\n",
    "\n",
    "# --- 2. Find the Best Calibration Factor ---\n",
    "print(\"\\nSearching for best calibration factor to target 90% coverage...\")\n",
    "best_factor = 1.0\n",
    "best_coverage_diff = abs(final_coverage - 0.90)\n",
    "\n",
    "for factor in np.arange(0.9, 1.2, 0.001):\n",
    "    center = (oof_lower + oof_upper) / 2\n",
    "    width = oof_upper - oof_lower\n",
    "    \n",
    "    new_lower = center - (width / 2) * factor\n",
    "    new_upper = center + (width / 2) * factor\n",
    "    \n",
    "    _, current_coverage = winkler_score(y_true_final, new_lower, new_upper, alpha=0.1, return_coverage=True)\n",
    "    \n",
    "    if abs(current_coverage - 0.90) < best_coverage_diff:\n",
    "        best_coverage_diff = abs(current_coverage - 0.90)\n",
    "        best_factor = factor\n",
    "\n",
    "print(f\"Best calibration factor found: {best_factor:.3f}\")\n",
    "\n",
    "\n",
    "# --- 3. Create Final Submission File ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "# Inverse transform the test set predictions\n",
    "test_lower = np.expm1(test_preds[:, 0])\n",
    "test_upper = np.expm1(test_preds[:, 1])\n",
    "\n",
    "# Apply the learned calibration factor\n",
    "print(f\"Applying calibration factor ({best_factor:.3f}) to test predictions...\")\n",
    "test_center = (test_lower + test_upper) / 2\n",
    "test_width = test_upper - test_lower\n",
    "calibrated_lower = test_center - (test_width / 2) * best_factor\n",
    "calibrated_upper = test_center + (test_width / 2) * best_factor\n",
    "\n",
    "# Final safety checks\n",
    "calibrated_upper = np.maximum(calibrated_lower, calibrated_upper)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': X_test_selected.index,\n",
    "    'pi_lower': calibrated_lower,\n",
    "    'pi_upper': calibrated_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_tuned_aggressive_final.csv', index=False)\n",
    "print(\"\\n'submission_tuned_aggressive_final.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7775a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-houseprice",
   "language": "python",
   "name": "kaggle-houseprice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
